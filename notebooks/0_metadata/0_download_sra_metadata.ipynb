{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Download-SRA-Metadata\" data-toc-modified-id=\"Download-SRA-Metadata-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Download SRA Metadata</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-date-of-last-metadata-pull\" data-toc-modified-id=\"Check-date-of-last-metadata-pull-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Check date of last metadata pull</a></span></li><li><span><a href=\"#Download-new-data\" data-toc-modified-id=\"Download-new-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Download new data</a></span></li></ul></li><li><span><a href=\"#Clean-metadata-file\" data-toc-modified-id=\"Clean-metadata-file-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Clean metadata file</a></span></li><li><span><a href=\"#Load-taxonomy-information\" data-toc-modified-id=\"Load-taxonomy-information-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load taxonomy information</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-parse-NCBI-taxonomy-file\" data-toc-modified-id=\"Download-and-parse-NCBI-taxonomy-file-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Download and parse NCBI taxonomy file</a></span></li><li><span><a href=\"#Read-taxonomy-files\" data-toc-modified-id=\"Read-taxonomy-files-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Read taxonomy files</a></span></li></ul></li><li><span><a href=\"#Add-taxonomy-information-to-metadata\" data-toc-modified-id=\"Add-taxonomy-information-to-metadata-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Add taxonomy information to metadata</a></span></li><li><span><a href=\"#Move-SRX-Experiment-ID-to-index\" data-toc-modified-id=\"Move-SRX-Experiment-ID-to-index-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Move SRX Experiment ID to index</a></span></li><li><span><a href=\"#Split-into-organism-specific-metadata\" data-toc-modified-id=\"Split-into-organism-specific-metadata-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Split into organism-specific metadata</a></span></li><li><span><a href=\"#Add-Additional-BioProject-and-PMID-data\" data-toc-modified-id=\"Add-Additional-BioProject-and-PMID-data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Add Additional BioProject and PMID data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from datetime import date\n",
    "import sys,wget,tarfile\n",
    "\n",
    "sys.path.append('../../modulome/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modulome.config import *\n",
    "from modulome.util import str2date,get_latest_sra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download SRA Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sra_dir = os.path.join(DATA_DIR,'sra_dumps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check date of last metadata pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last SRA pull on 2020-03-25\n"
     ]
    }
   ],
   "source": [
    "latest_pull = get_latest_sra(sra_dir)\n",
    "print('Last SRA pull on',latest_pull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download new data\n",
    "Uncomment this code to re-download the microbial SRA database  \n",
    "**TODO:** Simplify this to only download data since last pull  \n",
    "**TODO:** Save as a raw file, and then a processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# today = str(date.today())\n",
    "# metadata_file = os.path.join(sra_dir,'sra_metadata_{}_raw.csv'.format(today))\n",
    "\n",
    "# !esearch -db sra -query '\"bacteria\"[Organism] AND \"rna seq\"[Strategy] AND \"transcriptomic\"[Source]' | \\\n",
    "# efetch -db sra -format runinfo > \"$metadata_file\"\n",
    "\n",
    "# latest_pull = today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_file = os.path.join(sra_dir,'sra_metadata_'+str(latest_pull)+'_raw.csv')\n",
    "processed_file = os.path.join(sra_dir,'sra_metadata_'+str(latest_pull)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_metadata = pd.read_csv(metadata_file,low_memory=False)\n",
    "\n",
    "# # Remove duplicate title rows\n",
    "# DF_metadata = DF_metadata[DF_metadata.Run != 'Run']\n",
    "\n",
    "# # Reorder columns\n",
    "# priority_cols = ['ReleaseDate','TaxID','ScientificName','LibraryLayout','Platform','Model']\n",
    "# id_cols = ['Run','Experiment','SRAStudy','BioProject','ProjectID','Sample','BioSample','Submission']\n",
    "# exp_cols = ['LibraryStrategy','LibrarySelection','LibrarySource','Body_Site']\n",
    "# other_cols = ['CenterName','InsertSize','InsertDev']\n",
    "\n",
    "# keep_cols = priority_cols+id_cols+exp_cols+other_cols\n",
    "\n",
    "# DF_metadata = DF_metadata[keep_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load taxonomy information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and parse NCBI taxonomy file\n",
    "Uncomment this code to re-download the current NCBI taxonomy database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon_dir = os.path.join(DATA_DIR,'taxon_dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxon_url = 'ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz'\n",
    "\n",
    "# print('Downloading file...')\n",
    "# taxon_file = wget.download(taxon_url,out=DATA_DIR)\n",
    "\n",
    "# print('Extracting file...')\n",
    "# with tarfile.open(taxon_file) as tar:\n",
    "#     tar.extractall(taxon_dir)\n",
    "# os.remove(taxon_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# print('Loading taxonomy nodes...')\n",
    "# taxon_nodes = pd.read_csv(os.path.join(taxon_dir,'nodes.dmp'),\n",
    "#                           sep='\\t\\|\\t|\\t\\|',\n",
    "#                           engine='python',\n",
    "#                           usecols=[0,1,2],\n",
    "#                           names=['tax_id','parent_tax_id','rank'],\n",
    "#                           header=None)\n",
    "# taxon_nodes.set_index('tax_id',inplace=True)\n",
    "\n",
    "# print('Loading taxonomy names...')\n",
    "# taxon_names = pd.read_csv(os.path.join(taxon_dir,'names.dmp'),\n",
    "#                           sep='\\t\\|\\t|\\t\\|',\n",
    "#                           engine='python',\n",
    "#                           usecols=[0,1],\n",
    "#                           names=['tax_id','name_txt'],\n",
    "#                           header=None)\n",
    "\n",
    "# # Remove duplicate names and set tax_id as index\n",
    "# taxon_names = taxon_names.drop_duplicates('tax_id').set_index('tax_id')\n",
    "\n",
    "# # Save taxon nodes and names\n",
    "# print('Saving taxonomy files...')\n",
    "# taxon_nodes.to_csv(os.path.join(taxon_dir,'taxon_nodes.csv'))\n",
    "# taxon_names.to_csv(os.path.join(taxon_dir,'taxon_names.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read taxonomy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxon_nodes = pd.read_csv(os.path.join(taxon_dir,'taxon_nodes.csv'),index_col=0)\n",
    "# taxon_names = pd.read_csv(os.path.join(taxon_dir,'taxon_names.csv'),index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dict mapping tax_id to name\n",
    "# tax_name_dict = taxon_names.name_txt.to_dict()\n",
    "# tax_name_dict[-1] = 'No Tax ID'\n",
    "\n",
    "# taxon_nodes = pd.concat([taxon_nodes,taxon_names],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dictionary to speed up lookup of parent tax ID and rank\n",
    "# parent_tax_dict = taxon_nodes.parent_tax_id.to_dict()\n",
    "# tax_rank_dict = taxon_nodes['rank'].to_dict()\n",
    "\n",
    "# # Add cyanobacteria as a \"class\"\n",
    "# tax_rank_dict[1117] = 'class'\n",
    "\n",
    "# # Add name 'superkingdom' to root taxon\n",
    "# tax_rank_dict[0] = 'superkingdom'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add taxonomy information to metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modulome.taxon import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure that all files have an organism ID\n",
    "# assert(DF_metadata.TaxID.notnull().all())\n",
    "\n",
    "# # Change tax IDs to ints\n",
    "# DF_metadata.TaxID = DF_metadata.TaxID.astype(int)\n",
    "\n",
    "# # Update species and class taxonomies\n",
    "# DF_metadata['species_tax_id'] = DF_metadata.TaxID.apply(get_species,args=(tax_rank_dict,parent_tax_dict))\n",
    "# DF_metadata['class_tax_id'] = DF_metadata.TaxID.apply(get_class,args=(tax_rank_dict,parent_tax_dict))\n",
    "# DF_metadata['strain_tax_id'] = DF_metadata.TaxID.apply(get_strain,args=(tax_rank_dict,parent_tax_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add species names\n",
    "# tax2species = get_tax_dict(DF_metadata.species_tax_id.unique(),reformat=False)\n",
    "# tax2species[-1] = 'No species'\n",
    "# tax2species[0] = 'No species'\n",
    "\n",
    "# DF_metadata['species'] = [tax2species[tax] for tax in DF_metadata['species_tax_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move SRX Experiment ID to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get number of unique experiments\n",
    "# num_exp = len(DF_metadata.Experiment.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Figure out which metadata columns vary across runs within an experiment\n",
    "# for col in DF_metadata.columns:\n",
    "#     if num_exp != len(DF_metadata[[col,'Experiment']].drop_duplicates()):\n",
    "#         print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Just take the last ReleaseDate and last Submission number\n",
    "\n",
    "# agg_rules = {col:';'.join if col == 'Run' else\n",
    "#              'last' for col in DF_metadata.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_exp_metadata = DF_metadata.groupby('Experiment').agg(agg_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reorder metadata columns\n",
    "# columns = DF_exp_metadata.columns[:3].tolist()+DF_exp_metadata.columns[-4:].tolist()+DF_exp_metadata.columns[3:-4].tolist()\n",
    "# DF_exp_metadata = DF_exp_metadata[columns]\n",
    "\n",
    "# # Set Run as ID column\n",
    "# DF_exp_metadata.set_index('Experiment',inplace=True)\n",
    "\n",
    "# # Save file\n",
    "# DF_exp_metadata.to_csv(processed_file,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into organism-specific metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file\n",
    "DF_exp_metadata = pd.read_csv(processed_file,sep='\\t',index_col=0,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name,group in DF_exp_metadata.groupby('species'):\n",
    "    if len(group) > 250 and name != 'No species':\n",
    "        species = name.replace(' ','_')\n",
    "        species_dir = os.path.join(DATA_DIR,'organisms',species)\n",
    "        \n",
    "        # Make species directory if it doesn't already exist\n",
    "        if not os.path.isdir(species_dir):\n",
    "            os.mkdir(species_dir)\n",
    "        group.to_csv(os.path.join(species_dir,species+'.tsv'),sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Additional BioProject and PMID data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = '' # Tell NCBI your name\n",
    "Entrez.api_key = '' # Include an API key to perform more than 3 queries per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ncbi_id(id_str,db):\n",
    "    for id in id_str.split(';'):\n",
    "        handle = Entrez.esearch(db=db,term=id)\n",
    "        soup = BeautifulSoup(handle.read())\n",
    "        hits = int(soup.count.text)\n",
    "        if hits == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geo_ids(sra_id,experiment):\n",
    "    handle = Entrez.elink(dbfrom=\"sra\", db=\"gds\", id=sra_id, linkname=\"sra_gds\",retmode='xml')\n",
    "    record = handle.read()\n",
    "    soup = BeautifulSoup(record)\n",
    "    if soup.linksetdb:\n",
    "        geo_ids = [link.text for link in soup.linksetdb.find_all('id')]\n",
    "\n",
    "        gse_list = []\n",
    "        gsm_list = []\n",
    "        for geo_id in geo_ids:\n",
    "            if geo_id.startswith('2'):\n",
    "                gse_list.append('GSE'+geo_id[1:].lstrip('0'))\n",
    "            elif geo_id.startswith('3'):\n",
    "                gsm_list.append('GSM'+geo_id[1:].lstrip('0'))\n",
    "            else:\n",
    "                warnings.warn('Cannot identify GEO ID '+geo_id+' in '+experiment)\n",
    "        if len(gse_list) > 1:\n",
    "            warnings.warn('Found >1 GEO series for '+experiment)\n",
    "        if len(gsm_list) > 1:\n",
    "            warnings.warn('Found >1 GEO sample for '+experiment)\n",
    "        gse = ';'.join(gse_list)\n",
    "        gsm = ';'.join(gsm_list)\n",
    "\n",
    "\n",
    "        # Test GEO IDs\n",
    "        if not test_ncbi_id(gse,'gds'):\n",
    "            warnings.warn('GEO ID '+gse+' is invalid for '+experiment)\n",
    "        if not test_ncbi_id(gsm,'gds'):\n",
    "            warnings.warn('GEO ID '+gsm+' is invalid for '+experiment)\n",
    "\n",
    "        return gse,gsm\n",
    "    else:\n",
    "        return None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NCBI ID for Experiment\n",
    "\n",
    "def add_ncbi_metadata(experiment):\n",
    "\n",
    "    new_md = pd.Series(name=experiment)\n",
    "\n",
    "    handle = Entrez.esearch(db = \"sra\", term = experiment)\n",
    "    soup = BeautifulSoup(handle.read())\n",
    "\n",
    "    ids = [id.text for id in soup.idlist()]\n",
    "    if len(ids) > 1:\n",
    "        warnings.warn('Found >1 ID for '+experiment)\n",
    "    sra_id = ids[0]\n",
    "    new_md['SRA ID'] = sra_id\n",
    "\n",
    "    if experiment.startswith('SRX'):\n",
    "        gse,gsm = get_geo_ids(sra_id,experiment)\n",
    "        new_md['GEO Series'] = gse\n",
    "        new_md['GEO Sample'] = gsm\n",
    "\n",
    "    # Get PMID\n",
    "    handle = Entrez.elink(dbfrom=\"sra\", db=\"pubmed\", id=sra_id, linkname=\"sra_pubmed\",retmode='xml')\n",
    "    soup = BeautifulSoup(handle.read())\n",
    "    if soup.linksetdb:\n",
    "        pmid = ';'.join([link.text for link in soup.linksetdb.find_all('id')])\n",
    "        new_md['PMID'] = pmid\n",
    "\n",
    "    # Get biosample\n",
    "    handle = Entrez.elink(dbfrom=\"sra\", db=\"biosample\", id=sra_id, linkname=\"sra_biosample\",retmode='xml')\n",
    "    soup = BeautifulSoup(handle.read())\n",
    "    biosample_list = [link.text for link in soup.linksetdb.find_all('id')]\n",
    "    if len(biosample_list) > 1:\n",
    "        warnings.warn('Multiple biosamples found for '+experiment+'. Only first biosample will be stored')\n",
    "    biosample = biosample_list[0]\n",
    "    new_md['Biosample'] = biosample\n",
    "\n",
    "    # Get biosample attributes\n",
    "    handle = Entrez.efetch(db='biosample',id=biosample)\n",
    "    soup = BeautifulSoup(handle.read())\n",
    "\n",
    "    for attr in soup.attributes():\n",
    "        new_md[attr['attribute_name']] = attr.text\n",
    "    \n",
    "    return new_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for organism in os.listdir(os.path.join(DATA_DIR,'organisms')):\n",
    "    if organism in ['README.md','.ipynb_checkpoints']:\n",
    "        continue\n",
    "    elif organism in ['Mycolicibacterium_smegmatis','Acinetobacter_baumannii','Klebsiella_pneumoniae','Salmonella_enterica','Pseudomonas_putida',\n",
    "                      'Bacillus_subtilis','Staphylococcus_aureus','Pseudomonas_syringae_group_genomosp._3','Streptococcus_pyogenes',\n",
    "                      'Escherichia_coli','Listeria_monocytogenes','Prochlorococcus_marinus']:\n",
    "        continue\n",
    "    print(organism)\n",
    "    md_file = os.path.join(DATA_DIR,'organisms',organism,organism+'.tsv')\n",
    "    metadata = pd.read_csv(md_file,sep='\\t',index_col=0)\n",
    "    \n",
    "    new_md = []\n",
    "    for exp in tqdm(metadata.index):\n",
    "        if exp[:3] in ['SRX','DRX','ERX']:\n",
    "            try:\n",
    "                new_md.append(add_ncbi_metadata(exp))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    DF_new_data = pd.concat(new_md,axis=1,sort=False)\n",
    "    DF_final_additions = DF_new_data.T[['SRA ID','Biosample','GEO Series','GEO Sample','PMID','strain']]\n",
    "    DF_final_metadata = pd.concat([metadata,DF_final_additions],sort=False,axis=1)\n",
    "    \n",
    "    # Uncomment the following line to save TSVs\n",
    "    # DF_final_metadata.to_csv(md_file,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
